{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file은 크게 Data Preparation, Base Model, Add No Finding으로 나뉘어있음\n",
    "DataPreparation은 package import, 나눈 데이터들과 csv를 drive에서 불러오는 과정\n",
    "Base Model은 중권님이 보내줬던 것 그대로 실행\n",
    "Add No Finding은 여러가지 실험했던 것들\n",
    "    - 실험 한 결과는 csv와 weight로 저장함 (초반 실험은 기록이 안되어있음..)\n",
    "    - 실험 내용\n",
    "        - ForcedBCE어쩌구Loss부분에서 여러가지를 추가하고 빼보면서 결과 기록\n",
    "        - 1 그냥 BCEwithLogitLoss 사용\n",
    "        - 2 MSELoss 사용 -> 쓸데없음\n",
    "        - 3 allzero penalty 추가 (전부 0이 되지는 않게 함) -> 결과 애매 (하지만 논리상 필요)\n",
    "        - 4 nofinding penalty 추가 (nofinding이 1이면 나머지 0이 되게 함) -> 결과 애매 (하지만 논리상 필요)\n",
    "        - 5 focal weight 추가 (imbalanced 상황에서 BCE보완) -> 결과 괜찮게 나옴\n",
    "        - 5.1 focal weight에서 alpha, gamma term이 있는데 이걸 바꿔봄 -> 결과 애매\n",
    "        - 6 class weight 추가 (imbalanced 상황에서 BCE보완) -> 결과 애매\n",
    "        - 7 initial_alpha, initial_beta를 학습 가능하게 바꿈 -> beta가 음수로 가며 결과 엉망. alpha만 학습시킬 땐 학습안됨. 계속 0.1유지.\n",
    "    - bestmodel : allzero penalty, nofinding penalty, focal weight를 loss에 추가한 후 epoch 9번 돌린 모델\n",
    "    \n",
    "    - Dataset, Resnet18 define에선 Nofinding을 label로 추가하는 걸 실행\n",
    "    - DataLoader, Model, Test 안바꿈 (아마도...)\n",
    "    - Train에선 log 기록하는거 말곤 안바꿈 (아마도...)\n",
    "Hyperparameter Tuning\n",
    "    - optuna쓰는건데 최종모델에 대해서 돌리겠습니다.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBrL6ePft0Wd"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAUf8QFluBlE",
    "outputId": "5af1817d-6ebc-4684-95e5-948847aa7849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# prompt: drive mount\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GroupShuffleSplit\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import os\n",
    "import zipfile\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eys6266MXqFx"
   },
   "outputs": [],
   "source": [
    "## Zip file list 추출\n",
    "datapath = \"/content/drive/MyDrive/cxr_data\"  # Replace with the actual path\n",
    "\n",
    "def get_zip_files(directory):\n",
    "  zip_files = []\n",
    "  for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".zip\"):\n",
    "      zip_files.append(os.path.join(directory, filename))\n",
    "  return zip_files\n",
    "\n",
    "\n",
    "zip_files = sorted(get_zip_files(datapath))\n",
    "\n",
    "if zip_files:\n",
    "  print(\"Found the following .zip files:\")\n",
    "  for zip_file in zip_files:\n",
    "      if 'filtered' not in zip_file:\n",
    "          zip_files.remove(zip_file)\n",
    "      else:\n",
    "          print(zip_file)\n",
    "else:\n",
    "  print(\"No .zip files found in the specified directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fa85DtYvYOxT"
   },
   "outputs": [],
   "source": [
    "# UNZIP (약 6분)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 추출 디렉토리와 파일 리스트 설정\n",
    "extract_directory = \"/content/images\"\n",
    "os.makedirs(extract_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# ZIP 파일을 추출하는 함수\n",
    "def extract_zip(zip_file_path):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_directory)\n",
    "        return f\"Extracted all files from '{zip_file_path}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting '{zip_file_path}': {e}\"\n",
    "\n",
    "# 멀티스레딩 실행\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # 멀티스레딩으로 ZIP 파일 처리\n",
    "    results = list(tqdm(executor.map(extract_zip, zip_files), total=len(zip_files)))\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO6aaKmGOvjv"
   },
   "outputs": [],
   "source": [
    "## unizp된 파일 개수 확인\n",
    "def get_png_files(directory):\n",
    "  png_files = []\n",
    "  for filename in os.listdir(directory):\n",
    "    #for filename in os.listdir(dirname):\n",
    "      if filename.endswith(\".png\"):\n",
    "          png_files.append(os.path.join(directory, filename))\n",
    "  return png_files\n",
    "\n",
    "png_files = get_png_files('/content/images')\n",
    "print(len(png_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-CVFpmqyKJ5"
   },
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr0oOqRht5Fs"
   },
   "source": [
    "## Dataset, Resnet18 define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdIIonR5zjSn",
    "outputId": "6e407b8c-a892-400e-de60-904c8843cfd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "\n",
    "def get_labels(diseases):\n",
    "    labels = torch.zeros(len(class_names))\n",
    "    if diseases != 'No Finding':\n",
    "        for label_name in diseases.split('|'):\n",
    "            if label_name in class_names:\n",
    "                labels[class_names.index(label_name)] = 1\n",
    "    return labels\n",
    "\n",
    "class CXR14dataset(Dataset):\n",
    "    def __init__(self, img_dirs, df, augment=None):\n",
    "        self.img_dirs = img_dirs\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5123], [0.2307])\n",
    "        ])\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        diseases = self.df.iloc[idx, 1]\n",
    "        label = get_labels(diseases)\n",
    "\n",
    "        image = None\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                break\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image {img_name} not found in provided directories.\")\n",
    "\n",
    "        if self.augment:\n",
    "            image = self.augment(image)\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "\n",
    "        resnet = resnet18(weights=\"IMAGENET1K_V1\")  # Always use pretrained weights\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.4)\n",
    "        self.dropout3 = nn.Dropout(p=0.4)\n",
    "        self.dropout4 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT5fKb_zuCAx"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbQH-Gzi0x7C",
    "outputId": "3d4780b7-0393-48db-dacf-62da3fe5229d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-d2da0923aecb>:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(datapath+\"/train_df.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 72.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "          Dropout-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-21          [-1, 128, 28, 28]             256\n",
      "             ReLU-22          [-1, 128, 28, 28]               0\n",
      "           Conv2d-23          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "           Conv2d-25          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-26          [-1, 128, 28, 28]             256\n",
      "             ReLU-27          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-28          [-1, 128, 28, 28]               0\n",
      "           Conv2d-29          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 28, 28]             256\n",
      "             ReLU-31          [-1, 128, 28, 28]               0\n",
      "           Conv2d-32          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "             ReLU-34          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-35          [-1, 128, 28, 28]               0\n",
      "          Dropout-36          [-1, 128, 28, 28]               0\n",
      "           Conv2d-37          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-38          [-1, 256, 14, 14]             512\n",
      "             ReLU-39          [-1, 256, 14, 14]               0\n",
      "           Conv2d-40          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "           Conv2d-42          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-43          [-1, 256, 14, 14]             512\n",
      "             ReLU-44          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-45          [-1, 256, 14, 14]               0\n",
      "           Conv2d-46          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-47          [-1, 256, 14, 14]             512\n",
      "             ReLU-48          [-1, 256, 14, 14]               0\n",
      "           Conv2d-49          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-50          [-1, 256, 14, 14]             512\n",
      "             ReLU-51          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-52          [-1, 256, 14, 14]               0\n",
      "          Dropout-53          [-1, 256, 14, 14]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-56            [-1, 512, 7, 7]               0\n",
      "           Conv2d-57            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-58            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-59            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-60            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-61            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "           Conv2d-66            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-67            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-68            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-69            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 512, 1, 1]               0\n",
      "          Dropout-71                  [-1, 512]               0\n",
      "           Linear-72                    [-1, 8]           4,104\n",
      "================================================================\n",
      "Total params: 11,180,616\n",
      "Trainable params: 11,180,616\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 65.47\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 108.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## DataFrame Load\n",
    "train_df = pd.read_csv(datapath+\"/train_df.csv\")\n",
    "valid_df = pd.read_csv(datapath+\"/valid_df.csv\")\n",
    "test_df = pd.read_csv(datapath+\"/test_df.csv\")\n",
    "\n",
    "\n",
    "# CONSTANTS\n",
    "img_dirs = ['/content/images']\n",
    "\n",
    "class_names = ['Atelectasis',  'Consolidation',  'Effusion',\n",
    "                 'Infiltration', 'Mass', 'Nodule',\n",
    "                'Pleural_Thickening', 'Pneumothorax'\n",
    "               #'Hernia', 'Pneumonia', 'Fibrosis',  'Edema', 'Cardiomegaly', 'Emphysema',\n",
    "            ]\n",
    "\n",
    "num_classes = len(class_names)\n",
    "batch_size = 64\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "\n",
    "train_dataset = CXR14dataset(img_dirs=img_dirs, df=train_df)\n",
    "valid_dataset = CXR14dataset(img_dirs=img_dirs, df=valid_df)\n",
    "test_dataset = CXR14dataset(img_dirs=img_dirs, df=test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpvgSGByw5dd"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOZ0Xwb3u6Ut"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomResNet18(num_classes=num_classes).to(device)\n",
    "\n",
    "input_size = (3, 224, 224)\n",
    "summary(model, input_size=input_size, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yUhf6iv049R"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\"\"\"\n",
    "class_weights=train_class_counts.sum()/train_class_counts\n",
    "class_weights=class_weights/(class_weights[0])\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights = class_weights[1:]\n",
    "print(class_weights)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\"\"\"\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ16Ej7sw7Ob"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PDtRgxNF09Gx",
    "outputId": "1f3b591b-700c-479f-9490-7f1fe8c4233d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/1045 [00:00<?, ?batch/s]<ipython-input-14-684d8a738262>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n",
      "Epoch 1/20: 100%|██████████| 1045/1045 [03:36<00:00,  4.82batch/s]\n",
      "Validating Epoch 1/20: 100%|██████████| 132/132 [00:27<00:00,  4.81batch/s]\n",
      "<ipython-input-14-684d8a738262>:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.0539\n",
      "Epoch 1/20\n",
      "Train Loss: 0.2875, Train Accuracy: 0.4376\n",
      "Valid Loss: 0.2811, Valid Accuracy: 0.4423\n",
      "Validation F1 Score: 0.0539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:   0%|          | 0/1045 [00:00<?, ?batch/s]<ipython-input-14-684d8a738262>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n",
      "Epoch 2/20: 100%|██████████| 1045/1045 [03:40<00:00,  4.75batch/s]\n",
      "Validating Epoch 2/20: 100%|██████████| 132/132 [00:28<00:00,  4.64batch/s]\n",
      "<ipython-input-14-684d8a738262>:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.1037\n",
      "Epoch 2/20\n",
      "Train Loss: 0.2742, Train Accuracy: 0.4443\n",
      "Valid Loss: 0.2689, Valid Accuracy: 0.4473\n",
      "Validation F1 Score: 0.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:   0%|          | 0/1045 [00:00<?, ?batch/s]<ipython-input-14-684d8a738262>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n",
      "Epoch 3/20: 100%|██████████| 1045/1045 [03:33<00:00,  4.90batch/s]\n",
      "Validating Epoch 3/20: 100%|██████████| 132/132 [00:25<00:00,  5.12batch/s]\n",
      "<ipython-input-14-684d8a738262>:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.1061\n",
      "Epoch 3/20\n",
      "Train Loss: 0.2684, Train Accuracy: 0.4483\n",
      "Valid Loss: 0.2697, Valid Accuracy: 0.4406\n",
      "Validation F1 Score: 0.1061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:   0%|          | 0/1045 [00:00<?, ?batch/s]<ipython-input-14-684d8a738262>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n",
      "Epoch 4/20: 100%|██████████| 1045/1045 [03:31<00:00,  4.95batch/s]\n",
      "Validating Epoch 4/20: 100%|██████████| 132/132 [00:26<00:00,  5.04batch/s]\n",
      "<ipython-input-14-684d8a738262>:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.1492\n",
      "Epoch 4/20\n",
      "Train Loss: 0.2651, Train Accuracy: 0.4524\n",
      "Valid Loss: 0.2720, Valid Accuracy: 0.4511\n",
      "Validation F1 Score: 0.1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:   0%|          | 0/1045 [00:00<?, ?batch/s]<ipython-input-14-684d8a738262>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logdf = pd.concat([logdf, new_rows], ignore_index=True)\n",
      "Epoch 5/20:  28%|██▊       | 288/1045 [00:58<02:33,  4.93batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-684d8a738262>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_valid_f1 = 0.0\n",
    "save_path = \"best_model.pth\"\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    logdf = pd.DataFrame({'loss': [None], 'predict': [None], 'label': [None]})\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_preds += (preds == labels).all(dim=1).sum().item()\n",
    "        total_preds += preds.size(0)\n",
    "\n",
    "        ## print to csv\n",
    "        preds_array = preds.cpu().numpy()\n",
    "        labels_array = labels.cpu().numpy()\n",
    "\n",
    "        # loss 값을 64번 반복하여 각 row에 포함\n",
    "        loss_column = [loss.item()] * preds_array.shape[0]\n",
    "\n",
    "        # 새로운 DataFrame 생성\n",
    "        new_rows = pd.DataFrame({\n",
    "            'loss': loss_column,\n",
    "            'predict': list(preds_array),\n",
    "            'label': list(labels_array)\n",
    "        })\n",
    "        logdf = pd.concat([logdf, new_rows], ignore_index=True)\n",
    "    logdf.dropna(inplace=True)\n",
    "    logdf.to_csv(f'log_{epoch+1}_train.csv', index=False)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    logdf = pd.DataFrame({'loss': [None], 'predict': [None], 'label': [None]})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(valid_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct_preds += (preds == labels).all(dim=1).sum().item()\n",
    "            total_preds += preds.size(0)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_accuracy = correct_preds / total_preds\n",
    "\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        ## print to csv\n",
    "        preds_array = preds.cpu().numpy()\n",
    "        labels_array = labels.cpu().numpy()\n",
    "\n",
    "        # loss 값을 64번 반복하여 각 row에 포함\n",
    "        loss_column = [loss.item()] * preds_array.shape[0]\n",
    "\n",
    "        # 새로운 DataFrame 생성\n",
    "        new_rows = pd.DataFrame({\n",
    "            'loss': loss_column,\n",
    "            'predict': list(preds_array),\n",
    "            'label': list(labels_array)\n",
    "        })\n",
    "        logdf = pd.concat([logdf, new_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "        if f1 > best_valid_f1:\n",
    "            best_valid_f1 = f1\n",
    "            save_path = f\"resnet18_e{epoch+1}_{time.time()}.pth\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved with F1: {f1:.4f}\")\n",
    "\n",
    "    logdf.dropna(inplace=True)\n",
    "    logdf.to_csv(f'log_{epoch+1}_valid.csv', index=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POrw_3Zdw8co"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8eu_Xt7w3CE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Test Loop ---\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_preds += (preds == labels).all(dim=1).sum().item()\n",
    "        total_preds += preds.size(0)\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "        all_probs.append(torch.sigmoid(outputs).cpu())\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            class_correct[i] += ((preds[:, i] == labels[:, i]).sum()).item()\n",
    "            class_total[i] += labels[:, i].size(0)\n",
    "\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = correct_preds / total_preds\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')  # Macro-Averaged F1 Score\n",
    "\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "auc = roc_auc_score(all_labels, all_probs, average='macro', multi_class='ovr')  # Multi-class AUC (One-vs-Rest)\n",
    "\n",
    "class_accuracy = [class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "\n",
    "print(\"\\nTest Results\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC Score: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nClass-wise Accuracy:\")\n",
    "for i, accuracy in enumerate(class_accuracy):\n",
    "    print(f\"Class {class_names[i]}: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5z_Kg7GyPqI"
   },
   "source": [
    "# Add No Finding, Force at least one class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWovSSZryvJE"
   },
   "source": [
    "## Dataset, Resnet18 define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duZn5FYdyT-d",
    "outputId": "3da594f2-614d-4119-a7a6-aea7aafdc01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "\n",
    "# Add No findings\n",
    "def get_labels(diseases):\n",
    "    labels = torch.zeros(len(class_names))\n",
    "    for label_name in diseases.split('|'):\n",
    "        if label_name in class_names:\n",
    "            labels[class_names.index(label_name)] = 1\n",
    "    return labels\n",
    "\n",
    "class CXR14dataset(Dataset):\n",
    "    def __init__(self, img_dirs, df, augment=None):\n",
    "        self.img_dirs = img_dirs\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5123], [0.2307])\n",
    "        ])\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        diseases = self.df.iloc[idx, 1]\n",
    "        label = get_labels(diseases)\n",
    "\n",
    "        image = None\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                break\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image {img_name} not found in provided directories.\")\n",
    "\n",
    "        if self.augment:\n",
    "            image = self.augment(image)\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "\n",
    "        resnet = resnet18(weights=\"IMAGENET1K_V1\")  # Always use pretrained weights\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.4)\n",
    "        self.dropout3 = nn.Dropout(p=0.4)\n",
    "        self.dropout4 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDrPdSLNywwZ"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ib8EP2ryrGq",
    "outputId": "c92d52c3-7463-4df5-cb21-e721af54116f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-230-414e0d68e358>:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(datapath+\"/train_df.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64, 9])\n"
     ]
    }
   ],
   "source": [
    "## DataFrame Load\n",
    "train_df = pd.read_csv(datapath+\"/train_df.csv\")\n",
    "valid_df = pd.read_csv(datapath+\"/valid_df.csv\")\n",
    "test_df = pd.read_csv(datapath+\"/test_df.csv\")\n",
    "\n",
    "\n",
    "# CONSTANTS\n",
    "img_dirs = ['/content/images']\n",
    "\n",
    "class_names = ['Atelectasis',  'Consolidation',  'Effusion',\n",
    "                 'Infiltration', 'Mass', 'Nodule',\n",
    "                'Pleural_Thickening', 'Pneumothorax', 'No Finding'\n",
    "               #'Hernia', 'Pneumonia', 'Fibrosis',  'Edema', 'Cardiomegaly', 'Emphysema',\n",
    "            ]\n",
    "\n",
    "num_classes = len(class_names)\n",
    "batch_size = 64\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "\n",
    "train_dataset = CXR14dataset(img_dirs=img_dirs, df=train_df)\n",
    "valid_dataset = CXR14dataset(img_dirs=img_dirs, df=valid_df)\n",
    "test_dataset = CXR14dataset(img_dirs=img_dirs, df=test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpCEP5xTyy6J"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true,
    "id": "lelcdvS9yziq"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomResNet18(num_classes=num_classes).to(device)\n",
    "\n",
    "#input_size = (3, 224, 224)\n",
    "#summary(model, input_size=input_size, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "2qSohvJpXink"
   },
   "outputs": [],
   "source": [
    "train_df['combined_labels'].fillna(\"No Finding\", inplace=True)\n",
    "valid_df['combined_labels'].fillna(\"No Finding\", inplace=True)\n",
    "test_df['combined_labels'].fillna(\"No Finding\", inplace=True)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "train_class_counts = train_df['combined_labels'].str.split('|', expand=True).stack().value_counts()\n",
    "valid_class_counts = valid_df['combined_labels'].str.split('|', expand=True).stack().value_counts()\n",
    "test_class_counts = test_df['combined_labels'].str.split('|', expand=True).stack().value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.barplot(x=train_class_counts.index, y=train_class_counts.values/train_class_counts.sum(), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('Train Class Distribution')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=valid_class_counts.index, y=valid_class_counts.values/valid_class_counts.sum(), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('Validation Class Distribution')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=test_class_counts.index, y=test_class_counts.values/test_class_counts.sum(), palette='viridis', ax=axes[2])\n",
    "axes[2].set_title('Test Class Distribution')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "T6fOGJ66RDfl",
    "outputId": "c55fa420-f701-4f41-dece-ebd64fb17b96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <td>2.906976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Infiltration</th>\n",
       "      <td>5.315440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Effusion</th>\n",
       "      <td>7.970384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atelectasis</th>\n",
       "      <td>9.309091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nodule</th>\n",
       "      <td>16.987255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mass</th>\n",
       "      <td>18.805823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumothorax</th>\n",
       "      <td>20.580392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consolidation</th>\n",
       "      <td>22.973461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <td>32.445131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> float64</label>"
      ],
      "text/plain": [
       "No Finding             2.906976\n",
       "Infiltration           5.315440\n",
       "Effusion               7.970384\n",
       "Atelectasis            9.309091\n",
       "Nodule                16.987255\n",
       "Mass                  18.805823\n",
       "Pneumothorax          20.580392\n",
       "Consolidation         22.973461\n",
       "Pleural_Thickening    32.445131\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_over_tcc = train_class_counts.sum() / train_class_counts\n",
    "one_over_tcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "xPAHY7heXV__",
    "outputId": "a9e12734-b51e-4ba6-8bd3-0a1eb8fa23b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <td>0.190560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Infiltration</th>\n",
       "      <td>0.348442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Effusion</th>\n",
       "      <td>0.522481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atelectasis</th>\n",
       "      <td>0.610237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nodule</th>\n",
       "      <td>1.113562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mass</th>\n",
       "      <td>1.232774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumothorax</th>\n",
       "      <td>1.349102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consolidation</th>\n",
       "      <td>1.505974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <td>2.126868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> float64</label>"
      ],
      "text/plain": [
       "No Finding            0.190560\n",
       "Infiltration          0.348442\n",
       "Effusion              0.522481\n",
       "Atelectasis           0.610237\n",
       "Nodule                1.113562\n",
       "Mass                  1.232774\n",
       "Pneumothorax          1.349102\n",
       "Consolidation         1.505974\n",
       "Pleural_Thickening    2.126868\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_over_tcc = one_over_tcc / one_over_tcc.mean()\n",
    "one_over_tcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4Pn8gyhYw9t",
    "outputId": "8e13e5d5-b3cf-49a4-a12c-7ff5dd70e927"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.61, 1.51, 0.52, 0.35, 1.23, 1.11, 2.13, 1.35, 0.19]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = [np.round(one_over_tcc.loc[x],2) for x in class_names]\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "yoLuHpHA0GEv"
   },
   "outputs": [],
   "source": [
    "class ForcedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', initial_alpha=0.1, initial_beta=0.1, gamma_neg=4, gamma_pos=1, class_weights=class_weights):\n",
    "        super(ForcedBCEWithLogitsLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=reduction, weight=torch.tensor(class_weights))\n",
    "        self.initial_alpha = initial_alpha\n",
    "        #self.alpha = nn.Parameter(torch.tensor(initial_alpha, dtype=torch.float32))  # Learnable parameter\n",
    "        self.initial_beta = initial_beta\n",
    "        # self.beta = nn.Parameter(torch.tensor(initial_beta, dtype=torch.float32))  # Learnable parameter\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        #self.class_weight = class_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "\n",
    "        ## BCEWithLogitsLoss\n",
    "        bce_loss = self.bce_loss(logits, targets)\n",
    "\n",
    "\n",
    "        ## class probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "        ## MSELoss\n",
    "        # mse_loss = self.mse_loss(probs, targets)\n",
    "\n",
    "        ## Focal asymmetric Loss\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)   # 맞은 거의 맞은 확률 + 틀린 거의 틀린 확률\n",
    "        #gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets) #\n",
    "        gamma = 2\n",
    "        focal_weight = torch.mean((1 - p_t).pow(gamma))\n",
    "\n",
    "\n",
    "\n",
    "        ## All-Zero penalty\n",
    "        \"\"\"\n",
    "        # Regularization term: encourage at least one class to be close to 1 for each sample\n",
    "        max_probs = torch.max(probs, dim=1).values  # Max probability for each sample\n",
    "        reg_loss =  1 - max_probs  # Regularization term encourages max_probs to be close to 1\n",
    "        \"\"\"\n",
    "        zero_predictions = (probs.sum(dim=1) == 0).float()  # Rows where sum is 0\n",
    "        zero_penalty = zero_predictions.sum()  # Count rows with all zeros\n",
    "\n",
    "\n",
    "        ## Exclusive No Finding penalty\n",
    "        last_label_probs = probs[:, -1]  # Last column (last predicted label)\n",
    "        preceding_probs = probs[:, :-1]  # All columns except the last one\n",
    "\n",
    "        # Penalize if preceding values are not close to 0 when last_label is close to 1\n",
    "        penalty = torch.sum(preceding_probs * last_label_probs.unsqueeze(1))\n",
    "\n",
    "\n",
    "        # Combine BCE loss with the regularization term\n",
    "        total_loss = 0.5 * focal_weight * bce_loss + self.initial_alpha * zero_penalty / logits.size(0) + self.initial_beta * penalty / logits.size(0)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "criterion = ForcedBCEWithLogitsLoss()\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = optim.Adam(list(model.parameters()) + [criterion.alpha], lr=1e-3)\n",
    "\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjFohFrD17Mt"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Akxrx1Xd18ZY",
    "outputId": "0ee92fc7-cd85-4b6d-fd00-d553a6f0263d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1045/1045 [03:25<00:00,  5.09batch/s]\n",
      "Validating Epoch 1/20: 100%|██████████| 132/132 [00:26<00:00,  5.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.1324\n",
      "Epoch 1/20\n",
      "Train Loss: 0.0166, Train Accuracy: 0.1270\n",
      "Valid Loss: 0.0148, Valid Accuracy: 0.1942\n",
      "Validation F1 Score: 0.1324\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1045/1045 [03:25<00:00,  5.07batch/s]\n",
      "Validating Epoch 2/20: 100%|██████████| 132/132 [00:25<00:00,  5.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.1444\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0147, Train Accuracy: 0.1784\n",
      "Valid Loss: 0.0146, Valid Accuracy: 0.2965\n",
      "Validation F1 Score: 0.1444\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.06batch/s]\n",
      "Validating Epoch 3/20: 100%|██████████| 132/132 [00:26<00:00,  5.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.1897\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0141, Train Accuracy: 0.1996\n",
      "Valid Loss: 0.0145, Valid Accuracy: 0.3086\n",
      "Validation F1 Score: 0.1897\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.06batch/s]\n",
      "Validating Epoch 4/20: 100%|██████████| 132/132 [00:25<00:00,  5.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Train Loss: 0.0138, Train Accuracy: 0.2115\n",
      "Valid Loss: 0.0148, Valid Accuracy: 0.3168\n",
      "Validation F1 Score: 0.1690\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.07batch/s]\n",
      "Validating Epoch 5/20: 100%|██████████| 132/132 [00:25<00:00,  5.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "Train Loss: 0.0134, Train Accuracy: 0.2230\n",
      "Valid Loss: 0.0142, Valid Accuracy: 0.3145\n",
      "Validation F1 Score: 0.1834\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1045/1045 [03:27<00:00,  5.04batch/s]\n",
      "Validating Epoch 6/20: 100%|██████████| 132/132 [00:25<00:00,  5.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2131\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0131, Train Accuracy: 0.2370\n",
      "Valid Loss: 0.0139, Valid Accuracy: 0.3130\n",
      "Validation F1 Score: 0.2131\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.07batch/s]\n",
      "Validating Epoch 7/20: 100%|██████████| 132/132 [00:25<00:00,  5.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "Train Loss: 0.0127, Train Accuracy: 0.2474\n",
      "Valid Loss: 0.0147, Valid Accuracy: 0.3422\n",
      "Validation F1 Score: 0.1880\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.07batch/s]\n",
      "Validating Epoch 8/20: 100%|██████████| 132/132 [00:25<00:00,  5.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "Train Loss: 0.0124, Train Accuracy: 0.2570\n",
      "Valid Loss: 0.0147, Valid Accuracy: 0.3402\n",
      "Validation F1 Score: 0.2130\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 1045/1045 [03:27<00:00,  5.05batch/s]\n",
      "Validating Epoch 9/20: 100%|██████████| 132/132 [00:25<00:00,  5.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2385\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0120, Train Accuracy: 0.2708\n",
      "Valid Loss: 0.0138, Valid Accuracy: 0.3077\n",
      "Validation F1 Score: 0.2385\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.07batch/s]\n",
      "Validating Epoch 10/20: 100%|██████████| 132/132 [00:25<00:00,  5.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "Train Loss: 0.0115, Train Accuracy: 0.2820\n",
      "Valid Loss: 0.0135, Valid Accuracy: 0.2996\n",
      "Validation F1 Score: 0.2362\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.05batch/s]\n",
      "Validating Epoch 11/20: 100%|██████████| 132/132 [00:25<00:00,  5.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2577\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0110, Train Accuracy: 0.2959\n",
      "Valid Loss: 0.0150, Valid Accuracy: 0.3494\n",
      "Validation F1 Score: 0.2577\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.06batch/s]\n",
      "Validating Epoch 12/20: 100%|██████████| 132/132 [00:25<00:00,  5.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "Train Loss: 0.0104, Train Accuracy: 0.3172\n",
      "Valid Loss: 0.0146, Valid Accuracy: 0.3000\n",
      "Validation F1 Score: 0.2542\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.05batch/s]\n",
      "Validating Epoch 13/20: 100%|██████████| 132/132 [00:26<00:00,  5.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "Train Loss: 0.0099, Train Accuracy: 0.3308\n",
      "Valid Loss: 0.0146, Valid Accuracy: 0.2909\n",
      "Validation F1 Score: 0.2483\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 1045/1045 [03:27<00:00,  5.05batch/s]\n",
      "Validating Epoch 14/20: 100%|██████████| 132/132 [00:25<00:00,  5.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2595\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0093, Train Accuracy: 0.3444\n",
      "Valid Loss: 0.0171, Valid Accuracy: 0.3619\n",
      "Validation F1 Score: 0.2595\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.07batch/s]\n",
      "Validating Epoch 15/20: 100%|██████████| 132/132 [00:25<00:00,  5.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2624\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0085, Train Accuracy: 0.3657\n",
      "Valid Loss: 0.0177, Valid Accuracy: 0.3663\n",
      "Validation F1 Score: 0.2624\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.06batch/s]\n",
      "Validating Epoch 16/20: 100%|██████████| 132/132 [00:25<00:00,  5.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2713\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0077, Train Accuracy: 0.3874\n",
      "Valid Loss: 0.0173, Valid Accuracy: 0.3214\n",
      "Validation F1 Score: 0.2713\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 1045/1045 [03:25<00:00,  5.07batch/s]\n",
      "Validating Epoch 17/20: 100%|██████████| 132/132 [00:25<00:00,  5.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2852\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0069, Train Accuracy: 0.4075\n",
      "Valid Loss: 0.0177, Valid Accuracy: 0.3243\n",
      "Validation F1 Score: 0.2852\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 1045/1045 [03:27<00:00,  5.05batch/s]\n",
      "Validating Epoch 18/20: 100%|██████████| 132/132 [00:25<00:00,  5.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2876\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0061, Train Accuracy: 0.4299\n",
      "Valid Loss: 0.0190, Valid Accuracy: 0.3180\n",
      "Validation F1 Score: 0.2876\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.06batch/s]\n",
      "Validating Epoch 19/20: 100%|██████████| 132/132 [00:25<00:00,  5.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with F1: 0.2934\n",
      "Epoch 19/20\n",
      "Train Loss: 0.0063, Train Accuracy: 0.4536\n",
      "Valid Loss: 0.0195, Valid Accuracy: 0.3218\n",
      "Validation F1 Score: 0.2934\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 1045/1045 [03:26<00:00,  5.06batch/s]\n",
      "Validating Epoch 20/20: 100%|██████████| 132/132 [00:25<00:00,  5.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "Train Loss: 0.0053, Train Accuracy: 0.4527\n",
      "Valid Loss: 0.0209, Valid Accuracy: 0.2701\n",
      "Validation F1 Score: 0.2818\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "TITLE = \"az_nf_focal_classw2\" # allzero, nofinding, learnable alpha\n",
    "\n",
    "num_epochs = 20\n",
    "best_valid_f1 = 0.0\n",
    "save_path = \"best_model.pth\"\n",
    "\n",
    "output_folder = f\"/content/training_{TITLE}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    logdf_trn = pd.DataFrame(columns=['loss', 'outputs', 'predicts', 'label'])\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_preds += (preds == labels).all(dim=1).sum().item()\n",
    "        total_preds += preds.size(0)\n",
    "\n",
    "        ## print to csv\n",
    "        outputs_array = outputs.detach().cpu().numpy().round(2)\n",
    "        preds_array = preds.cpu().numpy()\n",
    "        labels_array = labels.cpu().numpy()\n",
    "\n",
    "        # loss 값을 64번 반복하여 각 row에 포함\n",
    "        loss_column = [loss.item()] * outputs_array.shape[0]\n",
    "\n",
    "        # 새로운 DataFrame 생성\n",
    "        new_rows = pd.DataFrame({\n",
    "            'loss': loss_column,\n",
    "            'outputs': list(outputs_array),\n",
    "            'predicts': list(preds_array),\n",
    "            'label': list(labels_array)\n",
    "        })\n",
    "        logdf_trn = pd.concat([logdf_trn, new_rows], ignore_index=True)\n",
    "    logdf_trn.to_csv(output_folder+ f'/log_{epoch+1}_train.csv', index=False)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    logdf_val = pd.DataFrame(columns=['loss', 'outputs', 'predicts', 'label'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(valid_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # Compute binary predictions\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \"\"\"\n",
    "            # Check all-zero predictions row-wise\n",
    "            row_sums = preds.sum(dim=1)  # Sum predictions for each row\n",
    "            zero_rows = (row_sums == 0)  # Identify rows where all predictions are 0\n",
    "\n",
    "            # Handle all-zero rows\n",
    "            if zero_rows.any():  # If any row is all zeros\n",
    "                argmax_indices = outputs.argmax(dim=1)  # Get argmax index for each row\n",
    "                preds[zero_rows] = torch.zeros_like(preds[zero_rows])  # Reset all-zero rows to zeros\n",
    "                preds[zero_rows, argmax_indices[zero_rows]] = 1  # Set the argmax index to 1 for affected rows\n",
    "            \"\"\"\n",
    "\n",
    "            correct_preds += (preds == labels).all(dim=1).sum().item()\n",
    "            total_preds += preds.size(0)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "            ## print to csv\n",
    "            outputs_array = outputs.cpu().numpy().round(2)\n",
    "            preds_array = preds.cpu().numpy()\n",
    "            labels_array = labels.cpu().numpy()\n",
    "\n",
    "            # loss 값을 64번 반복하여 각 row에 포함\n",
    "            loss_column = [loss.item()] * outputs_array.shape[0]\n",
    "\n",
    "            # 새로운 DataFrame 생성\n",
    "            new_rows = pd.DataFrame({\n",
    "                'loss': loss_column,\n",
    "                'outputs': list(outputs_array),\n",
    "                'predicts': list(preds_array),\n",
    "                'label': list(labels_array)\n",
    "            })\n",
    "            logdf_val = pd.concat([logdf_val, new_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_accuracy = correct_preds / total_preds\n",
    "\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "\n",
    "\n",
    "        if f1 > best_valid_f1:\n",
    "            best_valid_f1 = f1\n",
    "            save_path = output_folder+f\"/resnet18_e{epoch+1}_{time.time()}.pth\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved with F1: {f1:.4f}\")\n",
    "\n",
    "    logdf_val.to_csv(output_folder+ f'/log_{epoch+1}_valid.csv', index=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "    print(\"===============================\")\n",
    "    with open(output_folder+'/training_log.txt', 'a') as f:  # 'a' 모드로 파일에 추가 기록\n",
    "        f.write(f\"Epoch {epoch+1}/{num_epochs}\\n\")\n",
    "        f.write(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Validation F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(\"===============================\\n\")\n",
    "    #print(f\"Criterion Alpha:  {criterion.alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "eWnXM1ZwNWq1",
    "outputId": "6819cc5d-c407-44d2-f1cb-36308e14951f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/training_az_nf_focal_classw2.zip'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# 'folder_path' 폴더를 'archive_name.zip'으로 압축\n",
    "shutil.make_archive('training_az_nf_focal_classw2', 'zip', '/content/training_az_nf_focal_classw2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "XAkMCF2WNXYI"
   },
   "outputs": [],
   "source": [
    "!cp training_az_nf_focal_classw2.zip /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w8yJRaR-xJg"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv8Jqd6O-swC",
    "outputId": "caaa6456-e7f7-40e9-d6b4-d22c8626099d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 132/132 [00:28<00:00,  4.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results\n",
      "Test Loss: 0.0194, Test Accuracy: 0.2953\n",
      "Test F1 Score: 0.3170\n",
      "Test AUC Score: 0.7633\n",
      "\n",
      "Class-wise Accuracy:\n",
      "Class Atelectasis: 0.8365\n",
      "Class Consolidation: 0.9469\n",
      "Class Effusion: 0.8446\n",
      "Class Infiltration: 0.7415\n",
      "Class Mass: 0.9269\n",
      "Class Nodule: 0.9161\n",
      "Class Pleural_Thickening: 0.9591\n",
      "Class Pneumothorax: 0.9356\n",
      "Class No Finding: 0.6919\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       Atelectasis       0.39      0.31      0.35      1173\n",
      "     Consolidation       0.26      0.03      0.06       422\n",
      "          Effusion       0.50      0.62      0.55      1298\n",
      "      Infiltration       0.42      0.29      0.34      1960\n",
      "              Mass       0.41      0.23      0.29       558\n",
      "            Nodule       0.34      0.18      0.23       605\n",
      "Pleural_Thickening       0.23      0.06      0.09       306\n",
      "      Pneumothorax       0.46      0.30      0.36       518\n",
      "        No Finding       0.71      0.50      0.58      3666\n",
      "\n",
      "         micro avg       0.53      0.38      0.44     10506\n",
      "         macro avg       0.41      0.28      0.32     10506\n",
      "      weighted avg       0.51      0.38      0.43     10506\n",
      "       samples avg       0.40      0.39      0.38     10506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Test Loop ---\n",
    "best_path = \"/content/training_az_nf_focal/resnet18_e9_1733642727.2667074.pth\"\n",
    "model=CustomResNet18(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(best_path))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_preds += (preds == labels).all(dim=1).sum().item()\n",
    "        total_preds += preds.size(0)\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "        all_probs.append(torch.sigmoid(outputs).cpu())\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            class_correct[i] += ((preds[:, i] == labels[:, i]).sum()).item()\n",
    "            class_total[i] += labels[:, i].size(0)\n",
    "\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = correct_preds / total_preds\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')  # Macro-Averaged F1 Score\n",
    "\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "auc = roc_auc_score(all_labels, all_probs, average='macro', multi_class='ovr')  # Multi-class AUC (One-vs-Rest)\n",
    "\n",
    "class_accuracy = [class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "\n",
    "print(\"\\nTest Results\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC Score: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nClass-wise Accuracy:\")\n",
    "for i, accuracy in enumerate(class_accuracy):\n",
    "    print(f\"Class {class_names[i]}: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzsN9s8D-qUE"
   },
   "source": [
    "## Post Processing, Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Wp2pKLfVlu5-",
    "outputId": "edbead2e-6a59-4c4d-d9b2-8e17fd19d6f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "correct\n",
       "False    5802\n",
       "True     2626\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v20 = pd.read_csv(\"training_az_nf_simple/log_20_valid.csv\")\n",
    "v20['correct'] = v20['predicts'] == v20['label']\n",
    "v20['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 931
    },
    "id": "3knV7pZlmQmx",
    "outputId": "6a4ec188-31d6-4cfe-d90e-21443e305e5d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>predicts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 1. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 1. 0. 0. 0. 0.]</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 1. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 1. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 0. 1. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 1. 0. 1. 0. 0. 0. 0.]</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 1. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 1. 0. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 1. 1. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 1. 1. 0. 0. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 1. 0.]</th>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 1. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 1. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 1. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <th>[0. 0. 1. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 1. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 1. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 1. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 1. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 1. 1. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 1. 1. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 1. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 1. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 1. 1. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 1. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 1. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 1. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <th>[1. 0. 1. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "label                         predicts                    \n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]    1993\n",
       "[0. 0. 0. 1. 0. 0. 0. 0. 0.]  [0. 0. 0. 1. 0. 0. 0. 0. 0.]     236\n",
       "[0. 0. 1. 0. 0. 0. 0. 0. 0.]  [0. 0. 1. 0. 0. 0. 0. 0. 0.]     154\n",
       "[1. 0. 0. 0. 0. 0. 0. 0. 0.]  [1. 0. 0. 0. 0. 0. 0. 0. 0.]      58\n",
       "[0. 0. 1. 1. 0. 0. 0. 0. 0.]  [0. 0. 1. 1. 0. 0. 0. 0. 0.]      45\n",
       "[0. 0. 0. 0. 0. 0. 0. 1. 0.]  [0. 0. 0. 0. 0. 0. 0. 1. 0.]      34\n",
       "[0. 0. 0. 0. 1. 0. 0. 0. 0.]  [0. 0. 0. 0. 1. 0. 0. 0. 0.]      33\n",
       "[0. 0. 0. 0. 0. 1. 0. 0. 0.]  [0. 0. 0. 0. 0. 1. 0. 0. 0.]      21\n",
       "[1. 0. 1. 0. 0. 0. 0. 0. 0.]  [1. 0. 1. 0. 0. 0. 0. 0. 0.]      16\n",
       "[0. 0. 0. 1. 0. 1. 0. 0. 0.]  [0. 0. 0. 1. 0. 1. 0. 0. 0.]       5\n",
       "[0. 0. 1. 0. 1. 0. 0. 0. 0.]  [0. 0. 1. 0. 1. 0. 0. 0. 0.]       5\n",
       "[1. 0. 0. 1. 0. 0. 0. 0. 0.]  [1. 0. 0. 1. 0. 0. 0. 0. 0.]       4\n",
       "[0. 0. 0. 0. 0. 0. 1. 0. 0.]  [0. 0. 0. 0. 0. 0. 1. 0. 0.]       2\n",
       "[0. 0. 0. 0. 1. 1. 0. 0. 0.]  [0. 0. 0. 0. 1. 1. 0. 0. 0.]       2\n",
       "[1. 0. 0. 0. 0. 0. 0. 1. 0.]  [1. 0. 0. 0. 0. 0. 0. 1. 0.]       2\n",
       "[0. 0. 0. 1. 0. 0. 0. 1. 0.]  [0. 0. 0. 1. 0. 0. 0. 1. 0.]       2\n",
       "[0. 1. 0. 1. 0. 0. 0. 0. 0.]  [0. 1. 0. 1. 0. 0. 0. 0. 0.]       2\n",
       "[0. 0. 1. 0. 0. 0. 0. 1. 0.]  [0. 0. 1. 0. 0. 0. 0. 1. 0.]       2\n",
       "[0. 1. 0. 0. 0. 0. 0. 0. 0.]  [0. 1. 0. 0. 0. 0. 0. 0. 0.]       2\n",
       "[0. 1. 1. 0. 0. 0. 0. 0. 0.]  [0. 1. 1. 0. 0. 0. 0. 0. 0.]       2\n",
       "[0. 0. 0. 1. 1. 0. 0. 0. 0.]  [0. 0. 0. 1. 1. 0. 0. 0. 0.]       1\n",
       "[0. 0. 1. 0. 0. 1. 0. 0. 0.]  [0. 0. 1. 0. 0. 1. 0. 0. 0.]       1\n",
       "[0. 0. 1. 1. 0. 1. 0. 0. 0.]  [0. 0. 1. 1. 0. 1. 0. 0. 0.]       1\n",
       "[0. 1. 1. 1. 0. 0. 0. 0. 0.]  [0. 1. 1. 1. 0. 0. 0. 0. 0.]       1\n",
       "[1. 0. 1. 0. 0. 0. 0. 1. 0.]  [1. 0. 1. 0. 0. 0. 0. 1. 0.]       1\n",
       "[1. 0. 1. 1. 0. 0. 0. 0. 0.]  [1. 0. 1. 1. 0. 0. 0. 0. 0.]       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v20right = v20[v20['correct'] == True]\n",
    "v20right[['label', 'predicts']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "trjXI5lXl50l",
    "outputId": "6b4cd980-b4d3-41f0-979a-a45a893aebf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>predicts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <th>[0. 0. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 1. 0. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">[1. 1. 0. 0. 1. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 1. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">[1. 1. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 1. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>796 rows × 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "label                         predicts                    \n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 0.]    708\n",
       "[0. 0. 0. 1. 0. 0. 0. 0. 0.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]    359\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 1. 0. 0. 0. 0. 0.]    331\n",
       "[0. 0. 0. 1. 0. 0. 0. 0. 0.]  [0. 0. 0. 0. 0. 0. 0. 0. 0.]    206\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 1. 0. 0. 0. 0. 0. 0.]    156\n",
       "                                                             ... \n",
       "[1. 1. 0. 0. 0. 1. 0. 0. 0.]  [1. 0. 1. 1. 0. 0. 0. 0. 0.]      1\n",
       "[1. 1. 0. 0. 1. 0. 0. 0. 0.]  [0. 0. 0. 0. 0. 0. 0. 0. 0.]      1\n",
       "                              [0. 0. 0. 0. 1. 0. 0. 0. 0.]      1\n",
       "[1. 1. 0. 0. 0. 0. 0. 0. 0.]  [1. 0. 0. 0. 0. 0. 1. 0. 0.]      1\n",
       "                              [1. 0. 0. 1. 0. 0. 0. 0. 0.]      1\n",
       "Name: count, Length: 796, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v20wrong = v20[v20['correct'] == False]\n",
    "v20wrong[['label', 'predicts']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "DSQi_dNXmk9-",
    "outputId": "34adbaa0-be74-4dbf-e4a3-8bfe2a9c234e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>40986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>25831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "correct\n",
       "True     40986\n",
       "False    25831\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t20 = pd.read_csv(\"training_az_nf_simple/log_20_train.csv\")\n",
    "t20['correct'] = t20['predicts'] == t20['label']\n",
    "t20['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "5ogxW-PQmwOV",
    "outputId": "0727cb36-2d8b-4650-de17-0308e3f7a621"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>predicts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <td>24886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>4971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 1. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>2195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 1. 0.]</th>\n",
       "      <td>1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 1. 0. 0. 1. 0. 0.]</th>\n",
       "      <th>[1. 0. 0. 1. 0. 0. 1. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 1. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 0. 1. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 1. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[1. 0. 1. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 1. 1. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[1. 1. 1. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 1. 1. 0. 1. 1. 0. 0. 0.]</th>\n",
       "      <th>[1. 1. 1. 0. 1. 1. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "label                         predicts                    \n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]    24886\n",
       "[0. 0. 0. 1. 0. 0. 0. 0. 0.]  [0. 0. 0. 1. 0. 0. 0. 0. 0.]     4971\n",
       "[0. 0. 1. 0. 0. 0. 0. 0. 0.]  [0. 0. 1. 0. 0. 0. 0. 0. 0.]     2195\n",
       "[1. 0. 0. 0. 0. 0. 0. 0. 0.]  [1. 0. 0. 0. 0. 0. 0. 0. 0.]     1934\n",
       "[0. 0. 0. 0. 0. 0. 0. 1. 0.]  [0. 0. 0. 0. 0. 0. 0. 1. 0.]     1355\n",
       "                                                              ...  \n",
       "[1. 0. 0. 1. 0. 0. 1. 0. 0.]  [1. 0. 0. 1. 0. 0. 1. 0. 0.]        1\n",
       "[1. 0. 0. 1. 0. 1. 0. 0. 0.]  [1. 0. 0. 1. 0. 1. 0. 0. 0.]        1\n",
       "[1. 0. 1. 0. 0. 1. 0. 0. 0.]  [1. 0. 1. 0. 0. 1. 0. 0. 0.]        1\n",
       "[1. 1. 1. 0. 0. 1. 0. 0. 0.]  [1. 1. 1. 0. 0. 1. 0. 0. 0.]        1\n",
       "[1. 1. 1. 0. 1. 1. 0. 0. 0.]  [1. 1. 1. 0. 1. 1. 0. 0. 0.]        1\n",
       "Name: count, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t20right = t20[t20['correct'] == True]\n",
    "t20right[['label', 'predicts']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "a48VkrOcm1pO",
    "outputId": "23b9f871-2a8a-4c15-ce5a-354a34f5cec8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>predicts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>2885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">[0. 0. 0. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1. 1. 1. 0. 1. 0. 0. 0. 0.]</th>\n",
       "      <th>[0. 1. 1. 1. 0. 0. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 1. 0. 1. 1. 1. 0. 0.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 1. 0. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">[0. 0. 0. 0. 0. 0. 0. 0. 1.]</th>\n",
       "      <th>[0. 0. 0. 0. 0. 0. 1. 1. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 0. 1. 1. 0. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0. 0. 0. 0. 1. 0. 0. 1. 0.]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1811 rows × 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "label                         predicts                    \n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 0.]    2885\n",
       "[0. 0. 0. 1. 0. 0. 0. 0. 0.]  [0. 0. 0. 0. 0. 0. 0. 0. 0.]    1775\n",
       "                              [0. 0. 0. 0. 0. 0. 0. 0. 1.]     901\n",
       "[1. 0. 0. 0. 0. 0. 0. 0. 0.]  [0. 0. 0. 0. 0. 0. 0. 0. 0.]     746\n",
       "[0. 0. 0. 0. 0. 1. 0. 0. 0.]  [0. 0. 0. 0. 0. 0. 0. 0. 0.]     670\n",
       "                                                              ... \n",
       "[1. 1. 1. 0. 1. 0. 0. 0. 0.]  [0. 1. 1. 1. 0. 0. 0. 0. 0.]       1\n",
       "[0. 0. 1. 0. 1. 1. 1. 0. 0.]  [0. 0. 0. 0. 0. 1. 0. 0. 0.]       1\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 1. 1. 0.]       1\n",
       "                              [0. 0. 0. 0. 0. 1. 1. 0. 0.]       1\n",
       "                              [0. 0. 0. 0. 1. 0. 0. 1. 0.]       1\n",
       "Name: count, Length: 1811, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t20wrong = t20[t20['correct'] == False]\n",
    "t20wrong[['label', 'predicts']].value_counts()\n",
    "#000000 을 전부 0000001로 강제하면 accuracy높아질지도..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7KWuPe925Cn",
    "outputId": "cdc45a2b-3f11-4d99-a1bd-16e23778afe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "s = '[1,2,3,4]'\n",
    "lst = ast.literal_eval(s)\n",
    "print(lst)  # [1, 2, 3, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "collapsed": true,
    "id": "ijF-JsGE3ayW",
    "outputId": "8120f69b-b738-489e-de82-543aed5ca9c2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"v20\",\n  \"rows\": 8428,\n  \"fields\": [\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06195765911953106,\n        \"min\": 0.3355721533298492,\n        \"max\": 0.6561898589134216,\n        \"num_unique_values\": 132,\n        \"samples\": [\n          0.476635068655014,\n          0.4831573963165283,\n          0.4174405336380005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"outputs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8428,\n        \"samples\": [\n          \"[ -8.14 -11.09 -10.88  -7.56  -9.76  -7.32  -7.85 -13.09   6.21]\",\n          \"[-2.59 -4.28 -2.85 -2.56 -3.8  -5.01 -1.82 -7.63 -0.97]\",\n          \"[-2.47 -2.69  0.19  0.27 -6.85 -2.82 -5.19 -7.57 -4.74]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicts\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 69,\n        \"samples\": [\n          \"[1. 0. 1. 0. 1. 0. 0. 0. 0.]\",\n          \"[0. 0. 0. 0. 0. 0. 0. 0. 1.]\",\n          \"[0. 1. 0. 0. 0. 0. 0. 0. 1.]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 105,\n        \"samples\": [\n          \"[1. 0. 0. 0. 0. 0. 1. 1. 0.]\",\n          \"[0. 1. 0. 1. 0. 1. 0. 0. 0.]\",\n          \"[0. 0. 1. 1. 0. 1. 0. 0. 0.]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicts_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "v20"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9dcf6a38-4b62-4a14-9b10-fe3887ca36bc\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>outputs</th>\n",
       "      <th>predicts</th>\n",
       "      <th>label</th>\n",
       "      <th>correct</th>\n",
       "      <th>label_list</th>\n",
       "      <th>predicts_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.492546</td>\n",
       "      <td>[-13.35 -13.87 -12.45 -11.45 -15.38  -9.45 -13...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.492546</td>\n",
       "      <td>[-7.6  -5.41 -5.1  -2.65 -5.76 -4.15 -5.09 -7....</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.492546</td>\n",
       "      <td>[-11.53  -9.95  -4.27  -5.17 -12.22  -6.33  -6...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.492546</td>\n",
       "      <td>[ -9.81  -9.1   -6.82  -3.24  -7.43  -4.   -10...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.492546</td>\n",
       "      <td>[-1.01 -3.09  3.36 -0.1  -2.65 -2.02 -1.2  -3....</td>\n",
       "      <td>[0. 0. 1. 0. 0. 0. 0. 0. 0.]</td>\n",
       "      <td>[0. 0. 1. 1. 1. 1. 0. 0. 0.]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9dcf6a38-4b62-4a14-9b10-fe3887ca36bc')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9dcf6a38-4b62-4a14-9b10-fe3887ca36bc button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9dcf6a38-4b62-4a14-9b10-fe3887ca36bc');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-c19a28b5-a3b1-4970-92ae-c3c73b342f6a\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c19a28b5-a3b1-4970-92ae-c3c73b342f6a')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-c19a28b5-a3b1-4970-92ae-c3c73b342f6a button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       loss                                            outputs  \\\n",
       "0  0.492546  [-13.35 -13.87 -12.45 -11.45 -15.38  -9.45 -13...   \n",
       "1  0.492546  [-7.6  -5.41 -5.1  -2.65 -5.76 -4.15 -5.09 -7....   \n",
       "2  0.492546  [-11.53  -9.95  -4.27  -5.17 -12.22  -6.33  -6...   \n",
       "3  0.492546  [ -9.81  -9.1   -6.82  -3.24  -7.43  -4.   -10...   \n",
       "4  0.492546  [-1.01 -3.09  3.36 -0.1  -2.65 -2.02 -1.2  -3....   \n",
       "\n",
       "                       predicts                         label  correct  \\\n",
       "0  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]     True   \n",
       "1  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]     True   \n",
       "2  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 1. 0. 0. 0. 0. 0.]    False   \n",
       "3  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]     True   \n",
       "4  [0. 0. 1. 0. 0. 0. 0. 0. 0.]  [0. 0. 1. 1. 1. 1. 0. 0. 0.]    False   \n",
       "\n",
       "                                      label_list  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "2  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "4  [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                                   predicts_list  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "4  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v20['label_list'] = v20['label'].apply(lambda x: x.replace(\" \", \",\"))\n",
    "v20['predicts_list'] = v20['predicts'].apply(lambda x: x.replace(\" \", \",\"))\n",
    "\n",
    "v20['label_list'] = v20['label_list'].apply(lambda x: ast.literal_eval(x))\n",
    "v20['predicts_list'] = v20['predicts_list'].apply(lambda x: ast.literal_eval(x))\n",
    "v20.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "CUPrjpwl95dn",
    "outputId": "54a11f21-9218-4061-9910-d099449009c3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"t20\",\n  \"rows\": 66817,\n  \"fields\": [\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02365448627598925,\n        \"min\": 0.0876305103302002,\n        \"max\": 0.7904998660087585,\n        \"num_unique_values\": 1045,\n        \"samples\": [\n          0.1297607272863388,\n          0.1440620571374893,\n          0.1608878821134567\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"outputs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 66817,\n        \"samples\": [\n          \"[-2.79 -2.28 -5.97 -0.5  -1.43 -0.38 -3.46 -5.13 -1.88]\",\n          \"[-2.3   0.37 -3.78  3.03 -1.34 -5.47 -7.74 -8.91 -8.62]\",\n          \"[-5.08 -7.39  5.14 -1.78 -7.75 -4.11 -5.35 -1.01 -6.94]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicts\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 126,\n        \"samples\": [\n          \"[1. 0. 0. 0. 0. 1. 0. 1. 0.]\",\n          \"[0. 0. 0. 0. 1. 0. 1. 0. 0.]\",\n          \"[0. 1. 0. 0. 1. 0. 0. 1. 0.]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 118,\n        \"samples\": [\n          \"[1. 0. 0. 0. 1. 1. 0. 0. 0.]\",\n          \"[0. 1. 0. 0. 1. 1. 0. 0. 0.]\",\n          \"[1. 0. 1. 0. 0. 0. 0. 0. 0.]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicts_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "t20"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-cf57eb2d-1726-471c-88a1-c8e82d604ffd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>outputs</th>\n",
       "      <th>predicts</th>\n",
       "      <th>label</th>\n",
       "      <th>correct</th>\n",
       "      <th>label_list</th>\n",
       "      <th>predicts_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102575</td>\n",
       "      <td>[-5.   -7.47 -7.15 -4.09 -4.76 -5.41 -6.19 -9....</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.102575</td>\n",
       "      <td>[-4.23 -3.33 -2.1   0.23 -5.29 -2.82 -5.42 -5....</td>\n",
       "      <td>[0. 0. 0. 1. 0. 0. 0. 0. 0.]</td>\n",
       "      <td>[0. 0. 1. 0. 0. 0. 0. 0. 0.]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102575</td>\n",
       "      <td>[-5.78 -6.39 -8.4  -5.59 -8.06 -8.22 -9.58 -9....</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102575</td>\n",
       "      <td>[ -5.73  -7.21  -7.91  -6.    -6.39  -5.67 -10...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102575</td>\n",
       "      <td>[ -8.94  -7.1   -7.99  -3.44 -10.22  -8.15 -10...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf57eb2d-1726-471c-88a1-c8e82d604ffd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-cf57eb2d-1726-471c-88a1-c8e82d604ffd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-cf57eb2d-1726-471c-88a1-c8e82d604ffd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-44241988-6c5c-48c5-9f04-b29c80526ea7\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44241988-6c5c-48c5-9f04-b29c80526ea7')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-44241988-6c5c-48c5-9f04-b29c80526ea7 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       loss                                            outputs  \\\n",
       "0  0.102575  [-5.   -7.47 -7.15 -4.09 -4.76 -5.41 -6.19 -9....   \n",
       "1  0.102575  [-4.23 -3.33 -2.1   0.23 -5.29 -2.82 -5.42 -5....   \n",
       "2  0.102575  [-5.78 -6.39 -8.4  -5.59 -8.06 -8.22 -9.58 -9....   \n",
       "3  0.102575  [ -5.73  -7.21  -7.91  -6.    -6.39  -5.67 -10...   \n",
       "4  0.102575  [ -8.94  -7.1   -7.99  -3.44 -10.22  -8.15 -10...   \n",
       "\n",
       "                       predicts                         label  correct  \\\n",
       "0  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]     True   \n",
       "1  [0. 0. 0. 1. 0. 0. 0. 0. 0.]  [0. 0. 1. 0. 0. 0. 0. 0. 0.]    False   \n",
       "2  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]     True   \n",
       "3  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]     True   \n",
       "4  [0. 0. 0. 0. 0. 0. 0. 0. 1.]  [0. 0. 0. 0. 0. 0. 0. 0. 1.]     True   \n",
       "\n",
       "                                      label_list  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "\n",
       "                                   predicts_list  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t20['label_list'] = t20['label'].apply(lambda x: x.replace(\" \", \",\"))\n",
    "t20['predicts_list'] = t20['predicts'].apply(lambda x: x.replace(\" \", \",\"))\n",
    "\n",
    "t20['label_list'] = t20['label_list'].apply(lambda x: ast.literal_eval(x))\n",
    "t20['predicts_list'] = t20['predicts_list'].apply(lambda x: ast.literal_eval(x))\n",
    "t20.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdP8I3Qo2kNb",
    "outputId": "7787923d-a689-4ab3-ffdf-492ea3cded0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass AUC (ROC): 0.8273129536636494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = np.array(list(t20['label_list'].values))\n",
    "y_score = np.array(t20['predicts_list'].to_list())\n",
    "\n",
    "# 다중 클래스 ROC-AUC 계산\n",
    "# multi_class='ovr' 또는 'ovo' 선택 가능. average='weighted' or 'macro' 등으로 조정 가능.\n",
    "auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
    "\n",
    "print(\"Multiclass AUC (ROC):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "kTaeYrTt3-Yt",
    "outputId": "543f2d4a-e368-4c52-abc2-751576582022"
   },
   "outputs": [],
   "source": [
    "n_classes = 9\n",
    "\n",
    "# 각 클래스별 ROC, AUC 계산\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# micro-average ROC 곡선 (모든 클래스의 TP, FP를 누적)\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# macro-average ROC 계산\n",
    "# 모든 클래스 ROC AUC의 단순 평균\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# ROC 커브 그리기\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f'micro-average ROC curve (AUC = {roc_auc[\"micro\"]:.2f})',\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label=f'macro-average ROC curve (AUC = {roc_auc[\"macro\"]:.2f})',\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue']*3\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # 무작위 분류 기준선\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multiclass ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMdR6SKaI3QR"
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PlGhJelFI55R",
    "outputId": "938a9a79-ac29-49ce-8690-9c63bf19087c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "OSwGnQeLJyiB"
   },
   "outputs": [],
   "source": [
    "class ForcedBLL_simple(nn.Module):\n",
    "    def __init__(self, reduction='mean', initial_alpha=0.1, initial_beta=0.1):\n",
    "        super(ForcedBLL_simple, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        self.initial_alpha = initial_alpha\n",
    "        self.initial_beta = initial_beta\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "\n",
    "        ## BCEWithLogitsLoss\n",
    "        bce_loss = self.bce_loss(logits, targets)\n",
    "\n",
    "\n",
    "        ## class probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        ## All-Zero penalty\n",
    "        \"\"\"\n",
    "        # Regularization term: encourage at least one class to be close to 1 for each sample\n",
    "        max_probs = torch.max(probs, dim=1).values  # Max probability for each sample\n",
    "        reg_loss =  1 - max_probs  # Regularization term encourages max_probs to be close to 1\n",
    "        \"\"\"\n",
    "        zero_predictions = (probs.sum(dim=1) == 0).float()  # Rows where sum is 0\n",
    "        zero_penalty = zero_predictions.sum()  # Count rows with all zeros\n",
    "\n",
    "\n",
    "        ## Exclusive No Finding penalty\n",
    "        last_label_probs = probs[:, -1]  # Last column (last predicted label)\n",
    "        preceding_probs = probs[:, :-1]  # All columns except the last one\n",
    "\n",
    "        # Penalize if preceding values are not close to 0 when last_label is close to 1\n",
    "        penalty = torch.sum(preceding_probs * last_label_probs.unsqueeze(1))\n",
    "\n",
    "\n",
    "        # Combine BCE loss with the regularization term\n",
    "        total_loss = bce_loss + self.initial_alpha * zero_penalty / logits.size(0) + self.initial_beta * penalty / logits.size(0)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ForcedBLL_weighted(nn.Module):\n",
    "    def __init__(self, reduction='mean', initial_alpha=0.1, initial_beta=0.1, class_weights=class_weights):\n",
    "        super(ForcedBLL_weighted, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=reduction, weight=torch.tensor(class_weights))\n",
    "        self.initial_alpha = initial_alpha\n",
    "        self.initial_beta = initial_beta\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "\n",
    "        ## BCEWithLogitsLoss\n",
    "        bce_loss = self.bce_loss(logits, targets)\n",
    "\n",
    "\n",
    "        ## class probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        ## All-Zero penalty\n",
    "        \"\"\"\n",
    "        # Regularization term: encourage at least one class to be close to 1 for each sample\n",
    "        max_probs = torch.max(probs, dim=1).values  # Max probability for each sample\n",
    "        reg_loss =  1 - max_probs  # Regularization term encourages max_probs to be close to 1\n",
    "        \"\"\"\n",
    "        zero_predictions = (probs.sum(dim=1) == 0).float()  # Rows where sum is 0\n",
    "        zero_penalty = zero_predictions.sum()  # Count rows with all zeros\n",
    "\n",
    "\n",
    "        ## Exclusive No Finding penalty\n",
    "        last_label_probs = probs[:, -1]  # Last column (last predicted label)\n",
    "        preceding_probs = probs[:, :-1]  # All columns except the last one\n",
    "\n",
    "        # Penalize if preceding values are not close to 0 when last_label is close to 1\n",
    "        penalty = torch.sum(preceding_probs * last_label_probs.unsqueeze(1))\n",
    "\n",
    "\n",
    "        # Combine BCE loss with the regularization term\n",
    "        total_loss = bce_loss + self.initial_alpha * zero_penalty / logits.size(0) + self.initial_beta * penalty / logits.size(0)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "class ForcedBLL_focal(nn.Module):\n",
    "    def __init__(self, reduction='mean', initial_alpha=0.1, initial_beta=0.1, gamma_neg=4, gamma_pos=1):\n",
    "        super(ForcedBLL_focal, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        self.initial_alpha = initial_alpha\n",
    "        self.initial_beta = initial_beta\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "\n",
    "        ## BCEWithLogitsLoss\n",
    "        bce_loss = self.bce_loss(logits, targets)\n",
    "\n",
    "\n",
    "        ## class probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        ## Focal asymmetric Loss\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)   # 맞은 거의 맞은 확률 + 틀린 거의 틀린 확률\n",
    "        gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets) #\n",
    "        #gamma = 2\n",
    "        focal_weight = torch.mean((1 - p_t).pow(gamma))\n",
    "\n",
    "\n",
    "\n",
    "        ## All-Zero penalty\n",
    "        \"\"\"\n",
    "        # Regularization term: encourage at least one class to be close to 1 for each sample\n",
    "        max_probs = torch.max(probs, dim=1).values  # Max probability for each sample\n",
    "        reg_loss =  1 - max_probs  # Regularization term encourages max_probs to be close to 1\n",
    "        \"\"\"\n",
    "        zero_predictions = (probs.sum(dim=1) == 0).float()  # Rows where sum is 0\n",
    "        zero_penalty = zero_predictions.sum()  # Count rows with all zeros\n",
    "\n",
    "\n",
    "        ## Exclusive No Finding penalty\n",
    "        last_label_probs = probs[:, -1]  # Last column (last predicted label)\n",
    "        preceding_probs = probs[:, :-1]  # All columns except the last one\n",
    "\n",
    "        # Penalize if preceding values are not close to 0 when last_label is close to 1\n",
    "        penalty = torch.sum(preceding_probs * last_label_probs.unsqueeze(1))\n",
    "\n",
    "\n",
    "        # Combine BCE loss with the regularization term\n",
    "        total_loss = focal_weight * bce_loss + self.initial_alpha * zero_penalty / logits.size(0) + self.initial_beta * penalty / logits.size(0)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class ForcedBLL_weightedfocal(nn.Module):\n",
    "    def __init__(self, reduction='mean', initial_alpha=0.1, initial_beta=0.1, gamma_neg=4, gamma_pos=1, class_weights=class_weights):\n",
    "        super(ForcedBLL_weightedfocal, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=reduction, weight=torch.tensor(class_weights))\n",
    "        self.initial_alpha = initial_alpha\n",
    "        self.initial_beta = initial_beta\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "\n",
    "        ## BCEWithLogitsLoss\n",
    "        bce_loss = self.bce_loss(logits, targets)\n",
    "\n",
    "\n",
    "        ## class probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "        ## Focal asymmetric Loss\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)   # 맞은 거의 맞은 확률 + 틀린 거의 틀린 확률\n",
    "        gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets)\n",
    "        focal_weight = torch.mean((1 - p_t).pow(gamma))\n",
    "\n",
    "\n",
    "\n",
    "        ## All-Zero penalty\n",
    "        \"\"\"\n",
    "        # Regularization term: encourage at least one class to be close to 1 for each sample\n",
    "        max_probs = torch.max(probs, dim=1).values  # Max probability for each sample\n",
    "        reg_loss =  1 - max_probs  # Regularization term encourages max_probs to be close to 1\n",
    "        \"\"\"\n",
    "        zero_predictions = (probs.sum(dim=1) == 0).float()  # Rows where sum is 0\n",
    "        zero_penalty = zero_predictions.sum()  # Count rows with all zeros\n",
    "\n",
    "\n",
    "        ## Exclusive No Finding penalty\n",
    "        last_label_probs = probs[:, -1]  # Last column (last predicted label)\n",
    "        preceding_probs = probs[:, :-1]  # All columns except the last one\n",
    "\n",
    "        # Penalize if preceding values are not close to 0 when last_label is close to 1\n",
    "        penalty = torch.sum(preceding_probs * last_label_probs.unsqueeze(1))\n",
    "\n",
    "\n",
    "        # Combine BCE loss with the regularization term\n",
    "        total_loss = 0.5 * focal_weight * bce_loss + self.initial_alpha * zero_penalty / logits.size(0) + self.initial_beta * penalty / logits.size(0)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTODSDANJAn5",
    "outputId": "3a77dd86-a50e-4a90-a58c-735ba59e19ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomResNet18(num_classes=num_classes).to(device)\n",
    "best_path = \"/content/training_az_nf_focal/resnet18_e9_1733642727.2667074.pth\"\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWn0gQfFI9i6",
    "outputId": "ce5e2ca1-b718-48ea-b906-7d80f20791bf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import optuna\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, log=True)\n",
    "    loss_fn_name = trial.suggest_categorical('loss_fn', ['CrossEntropy', 'ForcedBLL_simple', 'ForcedBLL_weighted', 'ForcedBLL_focal', 'ForcedBLL_weightedfocal' ])\n",
    "    scheduler_step_size = trial.suggest_int('scheduler_step_size', 1, 10)\n",
    "    scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.1, 0.9)\n",
    "\n",
    "    # Data loading and preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CXR14dataset(img_dirs=img_dirs, df=train_df)\n",
    "    valid_dataset = CXR14dataset(img_dirs=img_dirs, df=valid_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cores, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "\n",
    "    # Model initialization\n",
    "    model = CustomResNet18(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Loss function\n",
    "    if loss_fn_name == 'CrossEntropy':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif loss_fn_name == 'ForcedBLL_simple':\n",
    "        criterion = ForcedBLL_simple()\n",
    "    elif loss_fn_name == 'ForcedBLL_weighted':\n",
    "        criterion = ForcedBLL_weighted()\n",
    "    elif loss_fn_name == 'ForcedBLL_focal':\n",
    "        criterion = ForcedBLL_focal()\n",
    "    elif loss_fn_name == 'ForcedBLL_weightedfocal':\n",
    "        criterion = ForcedBLL_weightedfocal()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        print(\"epoch: \", epoch+1)\n",
    "        for data, target in tqdm(train_loader, file=sys.stdout):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Validation performance (use your validation dataset here)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(valid_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Create a study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(f'  Value: {trial.value}')\n",
    "print('  Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EBrL6ePft0Wd",
    "qJ16Ej7sw7Ob"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
